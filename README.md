# 书名：《XXX》

[![GitHub stars](https://img.shields.io/github/stars/用户名/仓库名?style=social)](https://github.com/用户名/仓库名)

## 📚 书籍介绍
- **作者**: XX教授
- **出版社**: XXX出版社
- **出版时间**: 202X年X月
- **ISBN**: XXX-XXX-XXX
- **官方购买链接**: [点击购买](链接)

## 🎯 本书特色
- 特色1：...
- 特色2：...
- ...

## 📁 资源下载
- [配套代码下载](链接)
- [课件PPT下载](链接)
- [习题答案下载](链接)（如适用）
- [数据集下载](链接)

## 🔧 快速开始
```bash
# 克隆本仓库
git clone https://github.com/用户名/仓库名.git

# 安装依赖
pip install -r requirements.txt

# 运行示例代码
python examples/demo.py


📚 关于本书
为了更好地普及和传播大模型技术的最新进展与技术体系，我们编写了这本《大语言模型》。本书定位为大模型初学者的系统性技术指南，提供了完整的大模型技术框架和路线图。

🗓️ 编写历程
2023年3月：发布英文综述《A Survey of Large Language Models》（已更新至v14，95页正文，1064篇参考文献）

2023年8月：发布v10版本中文翻译版

2023年12月：启动中文书编写工作

2024年4月：完成初稿

2024年9月：正式出版（历时5个月修订完善）

🎯 本书特色
✅ 系统全面：为大模型初学者提供整体技术讲解，构建系统的大模型技术框架和路线图
✅ 基础友好：适用于具有深度学习基础的读者，作为基础的大模型参考书籍
✅ 精准提炼：基于经典论文、相关代码和学术教材，提炼核心概念、算法与模型
✅ 持续更新：本书出版是起点，将持续进行内容更新和完善

📖 内容定位
与英文综述文章的定位不同，本书更关注为大模型初学者提供整体的技术讲解，内容上进行了大范围的更新与重组，力图展现一个系统的大模型技术框架和路线图。

编者注：在图书编写过程中，我们深感自身能力与知识的局限性，尽管已经付出了巨大的努力，但仍难免会有遗漏或不足之处。本书的出版仅是一个起点，我们将编写此书的过程也作为一个自身的学习过程，希望能够通过本书与读者进行深入交流，向更多的行业同行学习。

🌟 推荐语
张宏江
北京智源人工智能研究院学术顾问委员会主任、美国国家工程院外籍院士

"本书的编者长期从事大模型技术的相关研究，曾组织研发了文澜、玉兰等一系列大模型，具有深厚的科研与实践积累。本书内容深入结合了编者在研发大模型过程中的第一手经验，全面覆盖了大模型技术的多方面知识，可以作为深入学习大模型技术的参考书籍，强烈推荐阅读！"

鄂维南
北京大学讲席教授、中国科学院院士

"本书的编写团队于2023年3月发布了学术界首篇大语言模型综述文章'A Survey of Large Language Models'，受到了广泛关注。在这篇经典综述文章基础上，编写团队对编写内容进行了精心组织与撰写，并且融入了其长期从事大模型技术的科研经验。本书具有重要的参考与学习价值，是一部值得推荐的大模型佳作。"

张亚勤
清华大学智能科学讲席教授、中国工程院外籍院士

"大模型作为一种快速兴起的人工智能技术，已经深刻地影响了未来的科技发展趋势。为了更好地推进大模型技术在我国的学习与普及，亟须有专业的中文技术图书进行系统介绍。本书是一部精心编写的大模型技术图书，涵盖了预训练、微调、对齐、提示工程等众多基础内容，能够为相关从业人员提供权威的、系统的学习参考，强烈推荐阅读。"

🎓 课程资源
为了帮助课程教学及传播大模型知识，《大语言模型》编写团队特别提供了相应的PDF课件：

课程目录
课程	对应章节	主要内容
第一课	第一、二章	初识大模型：语言模型发展历程、大模型技术基础、GPT和DeepSeek模型介绍
第二课	第五章	模型架构：Transformer模型介绍、模型详细配置、长上下文模型和新型架构
第三课	第四、六章	预训练：预训练之数据工程、预训练之具体流程、训练优化、模型参数量与训练效率估计
第四课	第七章	指令微调：指令微调与常见策略、轻量化微调
第五课	第八章	人类对齐：人类对齐之基础、人类对齐之进阶
第六课	第九章	解码与部署：大模型解码、解码效率分析与加速算法、模型压缩
第七课	第十章	提示学习：提示设计、上下文学习、思维链提示、检索增强生成
第八课	第十一章	复杂推理：规划与智能体、复杂推理与慢思考
其他章节	第十二章等	大模型资源、大模型评测
📧 PPT课件获取
申请条件
仅限将本书作为主要教材的课程使用

申请流程
发送邮件至：batmanflyatruc.edu.cn

邮件标题：《大语言模型》PPT课件- {姓名}- {机构}

邮件内容：

text
已将本纸质书籍列为主要授课书籍，课程中需要明确的说明（请说明在教学课件中或者教学大纲中说明）。
PPT课件仅用于课程教学。
课程名称：[请填写课程名称]
📢 重要声明
未经许可，不得二次传播和上网传播。

📄 引用格式
bibtex
@book{LLMBook,
  title = {大语言模型},
  publisher = {高等教育出版社},
  year = {2024},
  author = {赵鑫，李军毅，周昆，唐天一，文继荣},
  address = {北京},
  url = {https://llmbook-zh.github.io/}
}
👥 内容贡献
各章节负责人
章节	负责人	参与人
第三章	闵映乾、杨晨	李军毅、周昆
第四章	张君杰、侯宇蓬、周昆	-
第五章	董梓灿	田震、唐天一
第六章	唐天一、陈昱硕	-
第七章	唐天一	成晓雪
第八章	李军毅	陈志朋
第九章	陈昱硕、刘沛羽、唐天一	周昆
第十章	李军毅、汤昕宇、都一凡	王晓磊
第十一章	任瑞阳、蒋锦昊	李军毅
第十二章	张北辰、周昆	张高玮
第十三章	周昆	蒋锦昊、李依凡、刘子康、孙文奇、王禹溴、徐澜玲、杨锦霞、郑博文
其他贡献者
参与本书编写、校对的同学（按拼音字母排序）：

曹乾、曹展硕、陈杰、程伽雅琪、戴孙浩、邓欣、丁毅杰、冯雪扬、高泽峰、苟志斌、辜子惠、郭歌扬、何东楠、侯新铭、胡译文、李炳黔、李成远、李欣潼、刘恩泽、刘炯楠、刘子涵、罗文扬、梅朗、欧柯杉、彭涵、阮恺、苏炜航、孙一丁、汤奕如、王家鹏、王磊、王淑婷、姚峰、尹彦彬、詹玉梁、张景森、张良、朱天宇、朱余韬

🙏 特别感谢
算力资源支持：中国人民大学大型科学仪器共享平台

感谢老师：陈跃国、鲁蔚征、石源（衷心感谢三位老师的支持）

💬 反馈意见
反馈渠道
如果您有任何意见、评论以及建议（请先确认最新版本中是否已经修正）：

GitHub Issues：通过项目的Issues页面反馈

邮件联系：

batmanflyatqq.com

lijunyi at ruc.edu.cn

francis_kun_zhou at ruc.edu.cn

steventianiyitang at outlook.com

电子版本说明
为了方便阅读，我们提供之前的中文书电子版本：

下载链接1：[链接地址]

下载链接2：[链接地址]

请注意：由于后续修订和篇幅原因，大模型应用章节未纳入正式出版，一切内容以正式出版物为准。

📖 书籍信息
项目	内容
书名	大语言模型
作者	赵鑫，李军毅，周昆，唐天一，文继荣
出版社	高等教育出版社
出版年份	2024
出版地	北京
官方网址	https://llmbook-zh.github.io/
适用读者	具有深度学习基础的研究人员、学生、工程师
最后更新：2024年9月
项目维护：《大语言模型》编写团队
